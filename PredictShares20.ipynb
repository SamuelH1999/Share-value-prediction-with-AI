{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac4dcf2",
   "metadata": {},
   "source": [
    "# Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64238bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "import yfinance as yf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pickle\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69186c2e",
   "metadata": {},
   "source": [
    "# read predicting sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a46dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(stock_symbols, start_date, end_date, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # write column headings\n",
    "        f.write('Date,Open,High,Low,Close,Volume\\n')\n",
    "        \n",
    "        #for symbol in stock_symbols:\n",
    "            # load share data for the symbol (Share name) and the given time period\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Auswahl der gewünschten Spalten\n",
    "        stock_data.to_csv(f, header=False)  # Schreiben der Daten in die Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50633fe",
   "metadata": {},
   "source": [
    "# load time sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fdbb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startdatum: 2018-10-01, Enddatum: 2019-05-31\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/lastSequenceForPrediction.xlsx'\n",
    "\n",
    "# store index of the excel file in this txt file\n",
    "index_file_path = 'settings/lastSequenceForPrediction.txt'\n",
    "\n",
    "# read index from the from the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "\n",
    "# read excel file\n",
    "df_lastSeq = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if index is out of bounds\n",
    "if current_row_index >= len(df_lastSeq):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract data fom the actual row\n",
    "    start_date_lastSeq = df_lastSeq.loc[current_row_index, 'start_date']\n",
    "    end_date_lastSeq = df_lastSeq.loc[current_row_index, 'end_date']\n",
    "\n",
    "    # Output of the actual data \n",
    "    print(f'Startdatum: {start_date_lastSeq}, Enddatum: {end_date_lastSeq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074f3def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Enter symbols\n",
    "stock_symbols = ['ALV.DE', 'DBK.DE', 'VOW3.DE', 'BMW.DE', 'ADS.DE', 'BEI.DE', 'DTE.SG', 'SAP.DE', '1COV.DE', 'BAS.DE', 'EOAN.DE', 'RWE.DE']\n",
    "start_date = start_date_lastSeq\n",
    "end_date = end_date_lastSeq\n",
    "\n",
    "# load and store the data for every share\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}.csv'\n",
    "    download_stock_data(symbol, start_date, end_date, output_file)\n",
    "\n",
    "    # read CSV and select select the desired columns\n",
    "    df = pd.read_csv(output_file, usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Search CSV file for empty lines and remove them\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # filter the empty lines\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the file with the adjusted rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e1a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data frames for different symbols in the dictionary\n",
    "dfPre = {}\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # load CSV\n",
    "    dfPre[symbol] = pd.read_csv(f'stock_data_{symbol}.csv')\n",
    "\n",
    "    # Drop empty rows\n",
    "    dfPre[symbol].dropna(inplace=True)\n",
    "\n",
    "    # transform date to index\n",
    "    dfPre[symbol].set_index('Date', inplace=True)\n",
    "\n",
    "    # Check the number of rows\n",
    "    num_rows = dfPre[symbol].shape[0]\n",
    "\n",
    "    # If more than 60 rows, keep the last 60 rows\n",
    "    if num_rows > 60:\n",
    "        dfPre[symbol] = dfPre[symbol].tail(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63ed050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte vor der Behandlung für ALV.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für ALV.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für DBK.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für DBK.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für VOW3.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für VOW3.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BMW.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BMW.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für ADS.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für ADS.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BEI.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BEI.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für DTE.SG gefunden: False\n",
      "Fehlende Werte nach der Behandlung für DTE.SG gefunden: False\n",
      "Fehlende Werte vor der Behandlung für SAP.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für SAP.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für 1COV.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für 1COV.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BAS.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BAS.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für EOAN.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für EOAN.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für RWE.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für RWE.DE gefunden: False\n",
      "Länge des Datensatzes für ALV.DE: 60\n",
      "Länge des Datensatzes für DBK.DE: 60\n",
      "Länge des Datensatzes für VOW3.DE: 60\n",
      "Länge des Datensatzes für BMW.DE: 60\n",
      "Länge des Datensatzes für ADS.DE: 60\n",
      "Länge des Datensatzes für BEI.DE: 60\n",
      "Länge des Datensatzes für DTE.SG: 60\n",
      "Länge des Datensatzes für SAP.DE: 60\n",
      "Länge des Datensatzes für 1COV.DE: 60\n",
      "Länge des Datensatzes für BAS.DE: 60\n",
      "Länge des Datensatzes für EOAN.DE: 60\n",
      "Länge des Datensatzes für RWE.DE: 60\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing_values_beforePre = {symbol: dfPre[symbol].isnull().values.any() for symbol in stock_symbols}\n",
    "\n",
    "# missing values are replaced with teh mean of the row before and after the actual row\n",
    "for symbol in stock_symbols:\n",
    "    for column in dfPre[symbol].columns:\n",
    "        missing_valuesPre = dfPre[symbol][column].isnull()\n",
    "        dfPre[symbol].loc[missing_valuesPre, column] = (dfPre[symbol][column].shift() + dfPre[symbol][column].shift(-1)) / 2\n",
    "\n",
    "# Check if there are still any missing values \n",
    "missing_values_afterPre = {symbol: dfPre[symbol].isnull().values.any() for symbol in stock_symbols}\n",
    "\n",
    "# Output of the missing values before and after the Treatment\n",
    "for symbol in stock_symbols:\n",
    "    print(f\"Fehlende Werte vor der Behandlung für {symbol} gefunden:\", missing_values_beforePre[symbol])\n",
    "    print(f\"Fehlende Werte nach der Behandlung für {symbol} gefunden:\", missing_values_afterPre[symbol])\n",
    "\n",
    "# Output of the length of the data frame for all symbols\n",
    "for symbol in stock_symbols:\n",
    "    print(f\"Länge des Datensatzes für {symbol}:\", len(dfPre[symbol]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6894f0f",
   "metadata": {},
   "source": [
    "# Data normalization prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scaler\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "    \n",
    "# min-max scale factors\n",
    "min_value = scaler.data_min_\n",
    "max_value = scaler.data_max_\n",
    "# normalize the data for all symbols\n",
    "nfPre = {}\n",
    "nfPre_normalized = {}\n",
    "for symbol in stock_symbols:\n",
    "    # Copy the data fram and remove the column \"Date\" for every symbol\n",
    "    nfPre[symbol] = dfPre[symbol].copy()\n",
    "    \n",
    "    # Remove the index name \n",
    "    nfPre[symbol].index.name = None\n",
    "\n",
    "    # normalize the data for every symbol, except for the date (Index)\n",
    "    nfPre_normalized[symbol] = scaler.transform(nfPre[symbol])\n",
    "\n",
    "    # Create a new data frame with the normalized data and the original index for every symbol\n",
    "    nfPre[symbol] = pd.DataFrame(nfPre_normalized[symbol], columns=nfPre[symbol].columns, index=nfPre[symbol].index)\n",
    "\n",
    "    # Convert DataFrame to NumPy array for every symbol\n",
    "    nfPre[symbol] = nfPre[symbol].to_numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92ed14",
   "metadata": {},
   "source": [
    "# Daten preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a8ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store X_pre for every symbol in the dictionary\n",
    "X_pre_dict = {}\n",
    "\n",
    "# Iterate over each symbol\n",
    "for symbol in stock_symbols:\n",
    "    \n",
    "    nfPre_symbol = nfPre[symbol]\n",
    "    \n",
    "    # Define size for the current symbol\n",
    "    nfPre_size_symbol = len(nfPre_symbol)\n",
    "    \n",
    "    # Add external dimension\n",
    "    X_pre_symbol = np.expand_dims(nfPre_symbol[:nfPre_size_symbol], axis=0)\n",
    "    \n",
    "    # Transform to Tensor\n",
    "    X_pre_symbol_tensor = tf.convert_to_tensor(X_pre_symbol, dtype=tf.float32)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    X_pre_dict[symbol] = X_pre_symbol_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4c911",
   "metadata": {},
   "source": [
    "# Model importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c34d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = {}\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Load the stored model\n",
    "    loaded_models[symbol] = load_model(f'Predict20Days_{symbol}.h5.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa84085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALV.DE': <Sequential name=sequential_2, built=True>, 'DBK.DE': <Sequential name=sequential, built=True>, 'VOW3.DE': <Sequential name=sequential, built=True>, 'BMW.DE': <Sequential name=sequential, built=True>, 'ADS.DE': <Sequential name=sequential_1, built=True>, 'BEI.DE': <Sequential name=sequential, built=True>, 'DTE.SG': <Sequential name=sequential, built=True>, 'SAP.DE': <Sequential name=sequential, built=True>, '1COV.DE': <Sequential name=sequential, built=True>, 'BAS.DE': <Sequential name=sequential, built=True>, 'EOAN.DE': <Sequential name=sequential, built=True>, 'RWE.DE': <Sequential name=sequential, built=True>}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90458c92",
   "metadata": {},
   "source": [
    "# Prediction for the unkonown sequence and transform back into monetary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32404dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(symbol):\n",
    "    # Predict the current share(symbol)\n",
    "    predictions = loaded_models[symbol].predict(X_pre_dict[symbol])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0c7e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
      "Zurücktransformierte Vorhersagen für ALV.DE:\n",
      "[[1.8803723e+02 1.8639368e+02 1.8502802e+02 1.8077202e+02 6.3835994e+05]]\n",
      "180.77202\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Zurücktransformierte Vorhersagen für DBK.DE:\n",
      "[[ 7.5865121e+00 -3.9413073e+00 -1.3418933e+01  5.8028978e-01\n",
      "   1.8261014e+06]]\n",
      "0.5802898\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Zurücktransformierte Vorhersagen für VOW3.DE:\n",
      "[[1.3076035e+02 1.3058652e+02 1.3011418e+02 1.3213867e+02 9.5613125e+05]]\n",
      "132.13867\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step\n",
      "Zurücktransformierte Vorhersagen für BMW.DE:\n",
      "[[6.5791161e+01 6.6576927e+01 6.7476204e+01 6.6982307e+01 1.3963981e+06]]\n",
      "66.98231\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017DEB6AF760> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Zurücktransformierte Vorhersagen für ADS.DE:\n",
      "[[ 1.8111066e+02  1.8797861e+02  1.9331647e+02  1.9387920e+02\n",
      "  -1.4787889e+06]]\n",
      "193.8792\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017DECE24700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "Zurücktransformierte Vorhersagen für BEI.DE:\n",
      "[[1.01227844e+02 1.01013115e+02 9.99233246e+01 9.97654572e+01\n",
      "  7.18907188e+05]]\n",
      "99.76546\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
      "Zurücktransformierte Vorhersagen für DTE.SG:\n",
      "[[2.0896358e+01 1.7836683e+01 5.0550945e+01 2.7065741e+01 2.3830828e+05]]\n",
      "27.06574\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step\n",
      "Zurücktransformierte Vorhersagen für SAP.DE:\n",
      "[[1.0665377e+02 1.0640724e+02 1.0536245e+02 1.0590817e+02 1.6780709e+06]]\n",
      "105.90817\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "Zurücktransformierte Vorhersagen für 1COV.DE:\n",
      "[[ 5.0069168e+01  4.7445045e+01  4.3832115e+01  4.5664444e+01\n",
      "  -2.0512952e+05]]\n",
      "45.664444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Zurücktransformierte Vorhersagen für BAS.DE:\n",
      "[[6.3584213e+01 6.3448948e+01 6.2840702e+01 6.4907944e+01 1.4839040e+06]]\n",
      "64.90794\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n",
      "Zurücktransformierte Vorhersagen für EOAN.DE:\n",
      "[[4.1072128e+01 3.1860981e+01 2.1595226e+01 2.1903728e+01 1.3204421e+06]]\n",
      "21.903728\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
      "Zurücktransformierte Vorhersagen für RWE.DE:\n",
      "[[4.1571186e+01 4.2425339e+01 4.3159367e+01 4.5028751e+01 1.0499354e+06]]\n",
      "45.02875\n",
      "{'ALV.DE': {'prediction_20_days_ahead': 180.77202}, 'DBK.DE': {'prediction_20_days_ahead': 0.5802898}, 'VOW3.DE': {'prediction_20_days_ahead': 132.13867}, 'BMW.DE': {'prediction_20_days_ahead': 66.98231}, 'ADS.DE': {'prediction_20_days_ahead': 193.8792}, 'BEI.DE': {'prediction_20_days_ahead': 99.76546}, 'DTE.SG': {'prediction_20_days_ahead': 27.06574}, 'SAP.DE': {'prediction_20_days_ahead': 105.90817}, '1COV.DE': {'prediction_20_days_ahead': 45.664444}, 'BAS.DE': {'prediction_20_days_ahead': 64.90794}, 'EOAN.DE': {'prediction_20_days_ahead': 21.903728}, 'RWE.DE': {'prediction_20_days_ahead': 45.02875}}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary to store the prediction for every symbol\n",
    "predictions_dict = {}\n",
    "predictions = {}\n",
    "\n",
    "# Iterate over every symbol\n",
    "for symbol in stock_symbols:\n",
    "\n",
    "    # Prediction for the actual symbol\n",
    "    predictions[symbol] = prediction(symbol)\n",
    "\n",
    "    # Inverse transformation of the prediction\n",
    "    predictions_original_scale_symbol = scaler.inverse_transform(predictions[symbol])\n",
    "\n",
    "    # extract the prediction 20 days in the future\n",
    "    prediction_20_days_ahead_symbol = predictions_original_scale_symbol[0][-2]\n",
    "\n",
    "    # store the prediction for the current symbol\n",
    "    predictions_dict[symbol] = {\n",
    "        'prediction_20_days_ahead': prediction_20_days_ahead_symbol\n",
    "    }\n",
    "\n",
    "    # Output the values of the inverse transformation\n",
    "    print(f\"Zurücktransformierte Vorhersagen für {symbol}:\")\n",
    "    print(predictions_original_scale_symbol)\n",
    "    print(prediction_20_days_ahead_symbol)\n",
    "print(predictions_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a4f65",
   "metadata": {},
   "source": [
    "# read test CSV from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6f947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Startdatum: 2019-06-01, Enddatum: 2019-06-30\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/actualMonth_startEnd.xlsx'\n",
    "\n",
    "# path to file with the stored index\n",
    "index_file_path = 'settings/actualMonthIndex.txt'\n",
    "\n",
    "# read the current index form the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "print(current_row_index)\n",
    "# read Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if index is out of bounds\n",
    "if current_row_index >= len(df):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract the data from the current row\n",
    "    start_date2019 = df.loc[current_row_index, 'start_date']\n",
    "    end_date2019 = df.loc[current_row_index, 'end_date']\n",
    "\n",
    "    # Output the current data\n",
    "    print(f'Startdatum: {start_date2019}, Enddatum: {end_date2019}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fed4224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-01\n",
      "2019-06-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# time period\n",
    "start_date = start_date2019\n",
    "end_date = end_date2019\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "\n",
    "# load and store the data for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}_2019.csv'\n",
    "    download_stock_data(symbol, start_date, end_date, output_file)\n",
    "\n",
    "    # read file\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # filter the empty rows\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the data with the cleaned rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398e1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten für Symbol ALV.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2019-06-03  197.100006  199.380005  196.339996  199.160004  1057670\n",
      "2019-06-04  198.199997  203.550003  197.919998  203.399994  1164056\n",
      "2019-06-05  202.000000  203.250000  201.449997  202.300003   946091\n",
      "2019-06-06  202.399994  204.350006  201.850006  202.149994   955153\n",
      "2019-06-07  202.100006  204.699997  202.100006  204.300003  1012695\n",
      "Daten für Symbol DBK.DE:\n",
      "             Open   High    Low  Close    Volume\n",
      "Date                                            \n",
      "2019-06-03  6.000  6.010  5.801  5.975  22897840\n",
      "2019-06-04  5.950  6.241  5.940  6.241  19267974\n",
      "2019-06-05  6.250  6.283  6.082  6.138  12611897\n",
      "2019-06-06  6.123  6.204  5.863  5.958  20493968\n",
      "2019-06-07  5.982  6.028  5.927  5.966  10758861\n",
      "Daten für Symbol VOW3.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2019-06-03  138.419998  140.199997  138.080002  140.059998   801553\n",
      "2019-06-04  139.520004  144.960007  139.059998  144.660004  1502624\n",
      "2019-06-05  143.860001  144.820007  141.660004  142.779999   954747\n",
      "2019-06-06  142.000000  144.139999  140.820007  141.839996   865954\n",
      "2019-06-07  142.500000  142.860001  140.960007  141.880005   817419\n",
      "Daten für Symbol BMW.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2019-06-03  61.700001  61.880001  61.139999  61.639999  1838410\n",
      "2019-06-04  61.560001  63.709999  61.320000  63.430000  2567653\n",
      "2019-06-05  63.430000  63.590000  62.330002  63.020000  1484186\n",
      "2019-06-06  62.650002  63.259998  61.730000  62.340000  1947794\n",
      "2019-06-07  62.720001  62.790001  62.009998  62.320000  1249390\n",
      "Daten für Symbol ADS.DE:\n",
      "                  Open        High         Low       Close  Volume\n",
      "Date                                                              \n",
      "2019-06-03  254.750000  259.850006  254.300003  259.200012  537589\n",
      "2019-06-04  257.000000  258.649994  255.000000  258.000000  605898\n",
      "2019-06-05  257.450012  261.649994  256.299988  257.399994  501306\n",
      "2019-06-06  257.750000  261.600006  257.750000  260.500000  526545\n",
      "2019-06-07  261.200012  266.350006  261.200012  265.250000  495059\n",
      "Daten für Symbol BEI.DE:\n",
      "                  Open        High         Low       Close  Volume\n",
      "Date                                                              \n",
      "2019-06-03  103.000000  104.400002  103.000000  104.400002  468067\n",
      "2019-06-04  104.000000  104.800003  103.250000  103.400002  445537\n",
      "2019-06-05  103.199997  104.599998  102.900002  104.599998  312667\n",
      "2019-06-06  103.599998  104.949997  103.550003  104.500000  506079\n",
      "2019-06-07  105.000000  106.000000  104.750000  105.449997  369179\n",
      "Daten für Symbol DTE.SG:\n",
      "             Open   High    Low  Close  Volume\n",
      "Date                                          \n",
      "2019-06-03  13.69  13.69  13.69  13.69       0\n",
      "2019-06-04  13.69  13.69  13.69  13.69       0\n",
      "2019-06-05  13.69  13.69  13.69  13.69       0\n",
      "2019-06-06  13.69  13.69  13.69  13.69       0\n",
      "2019-06-07  13.69  13.69  13.69  13.69       0\n",
      "Daten für Symbol SAP.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2019-06-03  109.599998  111.379997  109.180000  111.220001  2280685\n",
      "2019-06-04  108.919998  111.139999  108.279999  111.139999  2637001\n",
      "2019-06-05  112.239998  113.919998  111.480003  111.959999  2471815\n",
      "2019-06-06  112.160004  112.800003  110.580002  111.379997  1885826\n",
      "2019-06-07  112.099998  114.239998  112.080002  113.720001  2354457\n",
      "Daten für Symbol 1COV.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2019-06-03  38.529999  39.290001  38.430000  39.290001  1083946\n",
      "2019-06-04  39.009998  41.660000  38.750000  41.180000  2232910\n",
      "2019-06-05  41.200001  41.639999  40.389999  40.840000  1667874\n",
      "2019-06-06  40.869999  42.380001  40.639999  41.080002  1866843\n",
      "2019-06-07  41.349998  42.099998  41.189999  41.369999  1467425\n",
      "Daten für Symbol BAS.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2019-06-03  58.830002  59.290001  58.500000  59.150002  2832153\n",
      "2019-06-04  59.020000  61.660000  58.750000  61.320000  4581537\n",
      "2019-06-05  61.320000  61.970001  60.709999  61.060001  2962065\n",
      "2019-06-06  61.060001  61.919998  60.369999  60.880001  2750179\n",
      "2019-06-07  61.400002  61.779999  60.980000  61.279999  2426526\n",
      "Daten für Symbol EOAN.DE:\n",
      "             Open   High    Low  Close    Volume\n",
      "Date                                            \n",
      "2019-06-03  9.328  9.438  9.291  9.416   7313784\n",
      "2019-06-04  9.402  9.498  9.398  9.453   6440030\n",
      "2019-06-05  9.440  9.594  9.430  9.593   8138924\n",
      "2019-06-06  9.574  9.790  9.563  9.755  11440740\n",
      "2019-06-07  9.749  9.994  9.749  9.925  13638114\n",
      "Daten für Symbol RWE.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2019-06-03  22.150000  22.240000  21.870001  22.240000  3503118\n",
      "2019-06-04  22.190001  22.530001  22.030001  22.330000  2507291\n",
      "2019-06-05  22.379999  22.690001  22.320000  22.629999  3338712\n",
      "2019-06-06  22.610001  23.090000  22.580000  23.070000  3577839\n",
      "2019-06-07  23.100000  23.670000  23.059999  23.469999  3444001\n"
     ]
    }
   ],
   "source": [
    "# load data for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    # create name for the CSV\n",
    "    csv_file = f'stock_data_{symbol}_2019.csv'\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Select the desired column\n",
    "    selected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    df = df[selected_columns]\n",
    "    \n",
    "    # show data\n",
    "    print(f\"Daten für Symbol {symbol}:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08973d9b",
   "metadata": {},
   "source": [
    "# Compare predictions to actual share values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe67132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergleich für Symbol ALV.DE:\n",
      "            Actual   Predicted\n",
      "2019-06-28   212.0  180.772018\n",
      "MSE für Symbol ALV.DE: 975.1868327728007\n",
      "\n",
      "\n",
      "Vergleich für Symbol DBK.DE:\n",
      "            Actual  Predicted\n",
      "2019-06-28    6.78    0.58029\n",
      "MSE für Symbol DBK.DE: 38.43640939991445\n",
      "\n",
      "\n",
      "Vergleich für Symbol VOW3.DE:\n",
      "                Actual   Predicted\n",
      "2019-06-28  148.220001  132.138672\n",
      "MSE für Symbol VOW3.DE: 258.6091535249725\n",
      "\n",
      "\n",
      "Vergleich für Symbol BMW.DE:\n",
      "               Actual  Predicted\n",
      "2019-06-28  65.089996  66.982307\n",
      "MSE für Symbol BMW.DE: 3.5808412847691216\n",
      "\n",
      "\n",
      "Vergleich für Symbol ADS.DE:\n",
      "            Actual   Predicted\n",
      "2019-06-28   271.5  193.879196\n",
      "MSE für Symbol ADS.DE: 6024.98918768228\n",
      "\n",
      "\n",
      "Vergleich für Symbol BEI.DE:\n",
      "                Actual  Predicted\n",
      "2019-06-28  105.550003  99.765457\n",
      "MSE für Symbol BEI.DE: 33.46097125112994\n",
      "\n",
      "\n",
      "Vergleich für Symbol DTE.SG:\n",
      "            Actual  Predicted\n",
      "2019-06-28   13.69  27.065741\n",
      "MSE für Symbol DTE.SG: 178.91044743133625\n",
      "\n",
      "\n",
      "Vergleich für Symbol SAP.DE:\n",
      "                Actual   Predicted\n",
      "2019-06-28  120.760002  105.908173\n",
      "MSE für Symbol SAP.DE: 220.57684035279132\n",
      "\n",
      "\n",
      "Vergleich für Symbol 1COV.DE:\n",
      "               Actual  Predicted\n",
      "2019-06-28  44.709999  45.664444\n",
      "MSE für Symbol 1COV.DE: 0.9109650389873423\n",
      "\n",
      "\n",
      "Vergleich für Symbol BAS.DE:\n",
      "               Actual  Predicted\n",
      "2019-06-28  63.919998  64.907944\n",
      "MSE für Symbol BAS.DE: 0.9760364228859544\n",
      "\n",
      "\n",
      "Vergleich für Symbol EOAN.DE:\n",
      "            Actual  Predicted\n",
      "2019-06-28   9.551  21.903728\n",
      "MSE für Symbol EOAN.DE: 152.58990988570537\n",
      "\n",
      "\n",
      "Vergleich für Symbol RWE.DE:\n",
      "            Actual  Predicted\n",
      "2019-06-28   21.67  45.028751\n",
      "MSE für Symbol RWE.DE: 545.6312621549623\n",
      "\n",
      "\n",
      "Gesamter Mean Squared Error für alle Vorhersagen: 702.8215714335447\n",
      "Gesamte Standardabweichung für alle Vorhersagen: 26.510782173175215\n",
      "Gesamte Standardabweichung pro Aktie: 2.209231847764601\n",
      "{'ALV.DE': {'prediction_20_days_ahead': 180.77202}, 'DBK.DE': {'prediction_20_days_ahead': 0.5802898}, 'VOW3.DE': {'prediction_20_days_ahead': 132.13867}, 'BMW.DE': {'prediction_20_days_ahead': 66.98231}, 'ADS.DE': {'prediction_20_days_ahead': 193.8792}, 'BEI.DE': {'prediction_20_days_ahead': 99.76546}, 'DTE.SG': {'prediction_20_days_ahead': 27.06574}, 'SAP.DE': {'prediction_20_days_ahead': 105.90817}, '1COV.DE': {'prediction_20_days_ahead': 45.664444}, 'BAS.DE': {'prediction_20_days_ahead': 64.90794}, 'EOAN.DE': {'prediction_20_days_ahead': 21.903728}, 'RWE.DE': {'prediction_20_days_ahead': 45.02875}}\n"
     ]
    }
   ],
   "source": [
    "# list to store the mse (mean squared error) for every symbol\n",
    "mse_list = []\n",
    "\n",
    "# list to store the actual and predicted value for every symbol\n",
    "all_actual_values = []\n",
    "all_predicted_values = []\n",
    "\n",
    "# Comparison of the actual and the predicted value for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    # create name for the CSV\n",
    "    csv_file = f'stock_data_{symbol}_2019.csv'\n",
    "    \n",
    "    # load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # set date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # extract last row\n",
    "    last_row = df.iloc[-1]\n",
    "    \n",
    "    # Extract the value from the \"Close\" column of the last row\n",
    "    actual_close = last_row['Close']\n",
    "    \n",
    "    # Calculate the mean squared error (mse)\n",
    "    mse = mean_squared_error([actual_close], [predictions_dict[symbol]['prediction_20_days_ahead']])\n",
    "    \n",
    "    # Add mse to the list\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    # Create a data frame with just one row for the actual and the predicted \"Close\" value\n",
    "    comparison_df = pd.DataFrame({'Actual': [actual_close], 'Predicted': predictions_dict[symbol]['prediction_20_days_ahead']}, index=[last_row.name])\n",
    "    \n",
    "    # Output of the comparison\n",
    "    print(f\"Vergleich für Symbol {symbol}:\")\n",
    "    print(comparison_df)\n",
    "    print(f\"MSE für Symbol {symbol}: {mse}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Add the actual and predicted value to the main list\n",
    "    all_actual_values.append(actual_close)\n",
    "    all_predicted_values.append(predictions_dict[symbol]['prediction_20_days_ahead'])\n",
    "    \n",
    "# calculate the mean squared error (mse) for all predictions\n",
    "total_mse = mean_squared_error(all_actual_values, all_predicted_values)\n",
    "standard_deviation = math.sqrt(total_mse)\n",
    "standard_deviation_share = standard_deviation / 12\n",
    "\n",
    "# Output of the mse for all values\n",
    "print(f\"Gesamter Mean Squared Error für alle Vorhersagen: {total_mse}\")\n",
    "print(f\"Gesamte Standardabweichung für alle Vorhersagen: {standard_deviation}\")\n",
    "print(f\"Gesamte Standardabweichung pro Aktie: {standard_deviation_share}\")\n",
    "\n",
    "# Define custom encoder class to handle float32 values\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Convert NumPy arrays in lists (if present)\n",
    "predictions_dict_serializable = {symbol: predictions.tolist() if isinstance(predictions, np.ndarray) else predictions for symbol, predictions in predictions_dict.items()}\n",
    "print(predictions_dict_serializable)\n",
    "# extract predictions for the mean variance model\n",
    "json_file = 'settings/predictions_dict.json'\n",
    "\n",
    "# Write array in JSON file\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(predictions_dict_serializable, f, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45dd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
