{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac4dcf2",
   "metadata": {},
   "source": [
    "# Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64238bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "import yfinance as yf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69186c2e",
   "metadata": {},
   "source": [
    "# read predicting sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a46dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(stock_symbols, start_date, end_date, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # write column headings\n",
    "        f.write('Date,Open,High,Low,Close,Volume\\n')\n",
    "        \n",
    "        #for symbol in stock_symbols:\n",
    "            # load share data for the symbol (Share name) and the given time period\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Auswahl der gewünschten Spalten\n",
    "        stock_data.to_csv(f, header=False)  # Schreiben der Daten in die Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50633fe",
   "metadata": {},
   "source": [
    "# load time sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fdbb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startdatum: 2023-10-01, Enddatum: 2024-03-31\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/lastSequenceForPrediction.xlsx'\n",
    "\n",
    "# store index of the excel file in this txt file\n",
    "index_file_path = 'settings/lastSequenceForPrediction.txt'\n",
    "\n",
    "# read index from the from the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "\n",
    "# read excel file\n",
    "df_lastSeq = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if index is out of bounds\n",
    "if current_row_index >= len(df_lastSeq):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract data fom the actual row\n",
    "    start_date_lastSeq = df_lastSeq.loc[current_row_index, 'start_date']\n",
    "    end_date_lastSeq = df_lastSeq.loc[current_row_index, 'end_date']\n",
    "\n",
    "    # Output of the actual data \n",
    "    print(f'Startdatum: {start_date_lastSeq}, Enddatum: {end_date_lastSeq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074f3def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Enter symbols\n",
    "stock_symbols = ['ALV.DE', 'DBK.DE', 'VOW3.DE', 'BMW.DE', 'ADS.DE', 'BEI.DE', 'DTE.SG', 'SAP.DE', '1COV.DE', 'BAS.DE', 'EOAN.DE', 'RWE.DE']\n",
    "start_date = start_date_lastSeq\n",
    "end_date = end_date_lastSeq\n",
    "\n",
    "# load and store the data for every share\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}.csv'\n",
    "    download_stock_data(symbol, start_date, end_date, output_file)\n",
    "\n",
    "    # read CSV and select select the desired columns\n",
    "    df = pd.read_csv(output_file, usecols=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "    \n",
    "    # Search CSV file for empty lines and remove them\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # filter the empty lines\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the file with the adjusted rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e1a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data frames for different symbols in the dictionary\n",
    "dfPre = {}\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # load CSV\n",
    "    dfPre[symbol] = pd.read_csv(f'stock_data_{symbol}.csv')\n",
    "\n",
    "    # Drop empty rows\n",
    "    dfPre[symbol].dropna(inplace=True)\n",
    "\n",
    "    # transform date to index\n",
    "    dfPre[symbol].set_index('Date', inplace=True)\n",
    "\n",
    "    # Check the number of rows\n",
    "    num_rows = dfPre[symbol].shape[0]\n",
    "\n",
    "    # If more than 60 rows, keep the last 60 rows\n",
    "    if num_rows > 60:\n",
    "        dfPre[symbol] = dfPre[symbol].tail(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63ed050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte vor der Behandlung für ALV.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für ALV.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für DBK.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für DBK.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für VOW3.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für VOW3.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BMW.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BMW.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für ADS.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für ADS.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BEI.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BEI.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für DTE.SG gefunden: False\n",
      "Fehlende Werte nach der Behandlung für DTE.SG gefunden: False\n",
      "Fehlende Werte vor der Behandlung für SAP.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für SAP.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für 1COV.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für 1COV.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für BAS.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für BAS.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für EOAN.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für EOAN.DE gefunden: False\n",
      "Fehlende Werte vor der Behandlung für RWE.DE gefunden: False\n",
      "Fehlende Werte nach der Behandlung für RWE.DE gefunden: False\n",
      "Länge des Datensatzes für ALV.DE: 60\n",
      "Länge des Datensatzes für DBK.DE: 60\n",
      "Länge des Datensatzes für VOW3.DE: 60\n",
      "Länge des Datensatzes für BMW.DE: 60\n",
      "Länge des Datensatzes für ADS.DE: 60\n",
      "Länge des Datensatzes für BEI.DE: 60\n",
      "Länge des Datensatzes für DTE.SG: 60\n",
      "Länge des Datensatzes für SAP.DE: 60\n",
      "Länge des Datensatzes für 1COV.DE: 60\n",
      "Länge des Datensatzes für BAS.DE: 60\n",
      "Länge des Datensatzes für EOAN.DE: 60\n",
      "Länge des Datensatzes für RWE.DE: 60\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing_values_beforePre = {symbol: dfPre[symbol].isnull().values.any() for symbol in stock_symbols}\n",
    "\n",
    "# missing values are replaced with teh mean of the row before and after the actual row\n",
    "for symbol in stock_symbols:\n",
    "    for column in dfPre[symbol].columns:\n",
    "        missing_valuesPre = dfPre[symbol][column].isnull()\n",
    "        dfPre[symbol].loc[missing_valuesPre, column] = (dfPre[symbol][column].shift() + dfPre[symbol][column].shift(-1)) / 2\n",
    "\n",
    "# Check if there are still any missing values \n",
    "missing_values_afterPre = {symbol: dfPre[symbol].isnull().values.any() for symbol in stock_symbols}\n",
    "\n",
    "# Output of the missing values before and after the Treatment\n",
    "for symbol in stock_symbols:\n",
    "    print(f\"Fehlende Werte vor der Behandlung für {symbol} gefunden:\", missing_values_beforePre[symbol])\n",
    "    print(f\"Fehlende Werte nach der Behandlung für {symbol} gefunden:\", missing_values_afterPre[symbol])\n",
    "\n",
    "# Output of the length of the data frame for all symbols\n",
    "for symbol in stock_symbols:\n",
    "    print(f\"Länge des Datensatzes für {symbol}:\", len(dfPre[symbol]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6894f0f",
   "metadata": {},
   "source": [
    "# Data normalization prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda529dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scaler\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "    \n",
    "# min-max scale factors\n",
    "min_value = scaler.data_min_\n",
    "max_value = scaler.data_max_\n",
    "# normalize the data for all symbols\n",
    "nfPre = {}\n",
    "nfPre_normalized = {}\n",
    "for symbol in stock_symbols:\n",
    "    # Copy the data fram and remove the column \"Date\" for every symbol\n",
    "    nfPre[symbol] = dfPre[symbol].copy()\n",
    "    \n",
    "    # Remove the index name \n",
    "    nfPre[symbol].index.name = None\n",
    "\n",
    "    # normalize the data for every symbol, except for the date (Index)\n",
    "    nfPre_normalized[symbol] = scaler.transform(nfPre[symbol])\n",
    "\n",
    "    # Create a new data frame with the normalized data and the original index for every symbol\n",
    "    nfPre[symbol] = pd.DataFrame(nfPre_normalized[symbol], columns=nfPre[symbol].columns, index=nfPre[symbol].index)\n",
    "\n",
    "    # Convert DataFrame to NumPy array for every symbol\n",
    "    nfPre[symbol] = nfPre[symbol].to_numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92ed14",
   "metadata": {},
   "source": [
    "# Daten preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a8ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store X_pre for every symbol in the dictionary\n",
    "X_pre_dict = {}\n",
    "\n",
    "# Iterate over each symbol\n",
    "for symbol in stock_symbols:\n",
    "    \n",
    "    nfPre_symbol = nfPre[symbol]\n",
    "    \n",
    "    # Define size for the current symbol\n",
    "    nfPre_size_symbol = len(nfPre_symbol)\n",
    "    \n",
    "    # Add external dimension\n",
    "    X_pre_symbol = np.expand_dims(nfPre_symbol[:nfPre_size_symbol], axis=0)\n",
    "    \n",
    "    # Transform to Tensor\n",
    "    X_pre_symbol_tensor = tf.convert_to_tensor(X_pre_symbol, dtype=tf.float32)\n",
    "    \n",
    "    # Store in the dictionary\n",
    "    X_pre_dict[symbol] = X_pre_symbol_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4c911",
   "metadata": {},
   "source": [
    "# Model importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c34d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = {}\n",
    "\n",
    "for symbol in stock_symbols:\n",
    "    # Load the stored model\n",
    "    loaded_models[symbol] = load_model(f'Predict20Days_{symbol}.h5.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa84085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALV.DE': <Sequential name=sequential_2, built=True>, 'DBK.DE': <Sequential name=sequential, built=True>, 'VOW3.DE': <Sequential name=sequential, built=True>, 'BMW.DE': <Sequential name=sequential, built=True>, 'ADS.DE': <Sequential name=sequential_1, built=True>, 'BEI.DE': <Sequential name=sequential, built=True>, 'DTE.SG': <Sequential name=sequential, built=True>, 'SAP.DE': <Sequential name=sequential, built=True>, '1COV.DE': <Sequential name=sequential, built=True>, 'BAS.DE': <Sequential name=sequential, built=True>, 'EOAN.DE': <Sequential name=sequential, built=True>, 'RWE.DE': <Sequential name=sequential, built=True>}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90458c92",
   "metadata": {},
   "source": [
    "# Prediction for the unkonown sequence and transform back into monetary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32404dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(symbol):\n",
    "    # Predict the current share(symbol)\n",
    "    predictions = loaded_models[symbol].predict(X_pre_dict[symbol])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d0c7e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "Zurücktransformierte Vorhersagen für ALV.DE:\n",
      "[[ 1.5027411e+02  1.5334561e+02  1.5421025e+02  1.4575539e+02\n",
      "  -2.9359368e+06]]\n",
      "145.75539\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step\n",
      "Zurücktransformierte Vorhersagen für DBK.DE:\n",
      "[[1.4493319e+01 1.3958506e+01 1.2314391e+01 1.4388221e+01 4.6299980e+06]]\n",
      "14.388221\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Zurücktransformierte Vorhersagen für VOW3.DE:\n",
      "[[8.3157845e+01 8.3239632e+01 8.1725540e+01 8.5346313e+01 3.9057795e+06]]\n",
      "85.34631\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Zurücktransformierte Vorhersagen für BMW.DE:\n",
      "[[8.918269e+01 8.994973e+01 8.838872e+01 8.720128e+01 6.269466e+06]]\n",
      "87.20128\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000026024CFE3B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "Zurücktransformierte Vorhersagen für ADS.DE:\n",
      "[[ 9.5883820e+01  1.0342735e+02  1.0486305e+02  1.0878242e+02\n",
      "  -1.0558294e+07]]\n",
      "108.78242\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000026024CFFA30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "Zurücktransformierte Vorhersagen für BEI.DE:\n",
      "[[1.03019722e+02 9.82030487e+01 1.02295074e+02 1.00752472e+02\n",
      "  9.31574100e+06]]\n",
      "100.75247\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "Zurücktransformierte Vorhersagen für DTE.SG:\n",
      "[[2.1689720e+01 2.3765873e+01 9.1609259e+00 2.0497587e+01 7.7212062e+05]]\n",
      "20.497587\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "Zurücktransformierte Vorhersagen für SAP.DE:\n",
      "[[8.3210686e+01 8.1599998e+01 7.3988037e+01 8.4601852e+01 1.3459488e+06]]\n",
      "84.60185\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n",
      "Zurücktransformierte Vorhersagen für 1COV.DE:\n",
      "[[5.0396545e+01 5.1521923e+01 4.9337036e+01 4.9830753e+01 2.3949178e+06]]\n",
      "49.830753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Zurücktransformierte Vorhersagen für BAS.DE:\n",
      "[[5.0613850e+01 5.1598976e+01 4.9702877e+01 5.0684361e+01 4.2405910e+06]]\n",
      "50.68436\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step\n",
      "Zurücktransformierte Vorhersagen für EOAN.DE:\n",
      "[[1.4786616e+01 1.5301644e+01 1.3405309e+01 1.3794801e+01 3.3523510e+06]]\n",
      "13.794801\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step\n",
      "Zurücktransformierte Vorhersagen für RWE.DE:\n",
      "[[3.2001835e+01 3.1200775e+01 3.1529230e+01 3.1657511e+01 2.5139560e+06]]\n",
      "31.65751\n",
      "{'ALV.DE': {'prediction_20_days_ahead': 145.75539}, 'DBK.DE': {'prediction_20_days_ahead': 14.388221}, 'VOW3.DE': {'prediction_20_days_ahead': 85.34631}, 'BMW.DE': {'prediction_20_days_ahead': 87.20128}, 'ADS.DE': {'prediction_20_days_ahead': 108.78242}, 'BEI.DE': {'prediction_20_days_ahead': 100.75247}, 'DTE.SG': {'prediction_20_days_ahead': 20.497587}, 'SAP.DE': {'prediction_20_days_ahead': 84.60185}, '1COV.DE': {'prediction_20_days_ahead': 49.830753}, 'BAS.DE': {'prediction_20_days_ahead': 50.68436}, 'EOAN.DE': {'prediction_20_days_ahead': 13.794801}, 'RWE.DE': {'prediction_20_days_ahead': 31.65751}}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary to store the prediction for every symbol\n",
    "predictions_dict = {}\n",
    "predictions = {}\n",
    "\n",
    "# Iterate over every symbol\n",
    "for symbol in stock_symbols:\n",
    "\n",
    "    # Prediction for the actual symbol\n",
    "    predictions[symbol] = prediction(symbol)\n",
    "\n",
    "    # Inverse transformation of the prediction\n",
    "    predictions_original_scale_symbol = scaler.inverse_transform(predictions[symbol])\n",
    "\n",
    "    # extract the prediction 20 days in the future\n",
    "    prediction_20_days_ahead_symbol = predictions_original_scale_symbol[0][-2]\n",
    "\n",
    "    # store the prediction for the current symbol\n",
    "    predictions_dict[symbol] = {\n",
    "        'prediction_20_days_ahead': prediction_20_days_ahead_symbol\n",
    "    }\n",
    "\n",
    "    # Output the values of the inverse transformation\n",
    "    print(f\"Zurücktransformierte Vorhersagen für {symbol}:\")\n",
    "    print(predictions_original_scale_symbol)\n",
    "    print(prediction_20_days_ahead_symbol)\n",
    "print(predictions_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a4f65",
   "metadata": {},
   "source": [
    "# read test CSV from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6f947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Startdatum: 2024-04-01, Enddatum: 2024-04-30\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/actualMonth_startEnd.xlsx'\n",
    "\n",
    "# path to file with the stored index\n",
    "index_file_path = 'settings/actualMonthIndex.txt'\n",
    "\n",
    "# read the current index form the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "print(current_row_index)\n",
    "# read Excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if index is out of bounds\n",
    "if current_row_index >= len(df):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract the data from the current row\n",
    "    start_date2019 = df.loc[current_row_index, 'start_date']\n",
    "    end_date2019 = df.loc[current_row_index, 'end_date']\n",
    "\n",
    "    # Output the current data\n",
    "    print(f'Startdatum: {start_date2019}, Enddatum: {end_date2019}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fed4224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-01\n",
      "2024-04-30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# time period\n",
    "start_date = start_date2019\n",
    "end_date = end_date2019\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "\n",
    "# load and store the data for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}_2019.csv'\n",
    "    download_stock_data(symbol, start_date, end_date, output_file)\n",
    "\n",
    "    # read file\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # filter the empty rows\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the data with the cleaned rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398e1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten für Symbol ALV.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2024-04-02  278.200012  280.000000  272.200012  273.899994  1013176\n",
      "2024-04-03  274.500000  276.600006  273.899994  274.399994   782102\n",
      "2024-04-04  274.100006  275.200012  272.200012  272.399994   690551\n",
      "2024-04-05  270.000000  270.200012  267.100006  268.799988   930874\n",
      "2024-04-08  266.799988  269.799988  266.200012  268.500000   766929\n",
      "Daten für Symbol DBK.DE:\n",
      "              Open    High     Low   Close   Volume\n",
      "Date                                               \n",
      "2024-04-02  14.600  14.886  14.480  14.568  9622210\n",
      "2024-04-03  14.580  14.916  14.548  14.872  7487699\n",
      "2024-04-04  14.922  15.044  14.892  14.972  7028089\n",
      "2024-04-05  14.700  14.778  14.584  14.762  6797982\n",
      "2024-04-08  14.768  14.928  14.718  14.894  4172636\n",
      "Daten für Symbol VOW3.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2024-04-02  122.199997  123.900002  121.800003  122.699997  1056617\n",
      "2024-04-03  123.000000  125.949997  122.699997  125.449997  1466950\n",
      "2024-04-04  125.300003  128.600006  124.800003  128.500000  1500099\n",
      "2024-04-05  126.449997  127.849998  125.599998  126.400002  1299458\n",
      "2024-04-08  127.000000  127.949997  126.099998  126.099998   884650\n",
      "Daten für Symbol BMW.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2024-04-02  107.199997  107.800003  105.800003  106.650002  1084212\n",
      "2024-04-03  106.949997  111.949997  106.599998  111.849998  1830218\n",
      "2024-04-04  111.900002  115.250000  111.750000  114.699997  1788276\n",
      "2024-04-05  112.800003  113.550003  111.900002  112.500000  1144372\n",
      "2024-04-08  113.199997  114.750000  112.750000  114.750000   827527\n",
      "Daten für Symbol ADS.DE:\n",
      "                  Open        High         Low       Close  Volume\n",
      "Date                                                              \n",
      "2024-04-02  206.899994  208.000000  201.500000  202.399994  482915\n",
      "2024-04-03  202.699997  203.800003  200.399994  201.399994  346901\n",
      "2024-04-04  200.800003  203.100006  200.300003  203.100006  268234\n",
      "2024-04-05  200.000000  203.399994  198.800003  201.600006  424601\n",
      "2024-04-08  201.000000  204.600006  200.899994  204.600006  300809\n",
      "Daten für Symbol BEI.DE:\n",
      "                  Open        High         Low       Close  Volume\n",
      "Date                                                              \n",
      "2024-04-02  134.649994  135.300003  132.399994  132.399994  273972\n",
      "2024-04-03  132.199997  132.350006  129.649994  129.850006  308810\n",
      "2024-04-04  129.750000  131.100006  129.449997  130.899994  165599\n",
      "2024-04-05  130.250000  131.050003  130.000000  130.350006  169609\n",
      "2024-04-08  129.949997  130.250000  129.100006  129.649994  217079\n",
      "Daten für Symbol DTE.SG:\n",
      "                 Open       High        Low      Close  Volume\n",
      "Date                                                          \n",
      "2024-04-02  22.490000  22.639999  22.309999  22.360001   78527\n",
      "2024-04-03  22.309999  22.549999  22.190001  22.299999   71380\n",
      "2024-04-04  22.320000  22.410000  22.139999  22.139999   29746\n",
      "2024-04-05  22.170000  22.240000  21.920000  21.990000   79751\n",
      "2024-04-08  22.059999  22.139999  21.879999  22.030001   93390\n",
      "Daten für Symbol SAP.DE:\n",
      "                  Open        High         Low       Close   Volume\n",
      "Date                                                               \n",
      "2024-04-02  181.000000  181.919998  177.059998  177.059998  1838833\n",
      "2024-04-03  178.320007  179.520004  176.559998  178.220001  1501774\n",
      "2024-04-04  177.919998  178.460007  176.339996  178.020004  1126985\n",
      "2024-04-05  175.399994  177.960007  174.779999  177.419998  2099406\n",
      "2024-04-08  176.699997  178.259995  176.039993  177.220001  1024504\n",
      "Daten für Symbol 1COV.DE:\n",
      "                 Open       High        Low      Close  Volume\n",
      "Date                                                          \n",
      "2024-04-02  50.720001  51.660000  50.299999  50.459999  655444\n",
      "2024-04-03  50.459999  51.380001  50.259998  51.340000  483507\n",
      "2024-04-04  51.419998  51.959999  51.220001  51.900002  494805\n",
      "2024-04-05  51.240002  51.939999  50.860001  51.220001  518059\n",
      "2024-04-08  51.060001  52.520000  50.939999  52.439999  689524\n",
      "Daten für Symbol BAS.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2024-04-02  53.450001  54.230000  53.209999  53.820000  2903536\n",
      "2024-04-03  53.580002  54.750000  53.570000  54.509998  2314791\n",
      "2024-04-04  54.430000  54.930000  54.270000  54.919998  1855813\n",
      "2024-04-05  54.209999  54.700001  53.349998  53.830002  2537273\n",
      "2024-04-08  54.040001  54.270000  53.529999  53.880001  1596462\n",
      "Daten für Symbol EOAN.DE:\n",
      "              Open    High     Low   Close   Volume\n",
      "Date                                               \n",
      "2024-04-02  12.825  12.935  12.755  12.760  5800004\n",
      "2024-04-03  12.785  12.785  12.505  12.670  4659867\n",
      "2024-04-04  12.650  12.750  12.615  12.615  3930438\n",
      "2024-04-05  12.555  12.615  12.330  12.385  6007232\n",
      "2024-04-08  12.360  12.480  12.315  12.430  3649939\n",
      "Daten für Symbol RWE.DE:\n",
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2024-04-02  31.260000  31.549999  30.930000  30.930000  2444701\n",
      "2024-04-03  31.000000  31.020000  30.299999  30.530001  3646176\n",
      "2024-04-04  30.540001  31.350000  30.540001  31.160000  2157766\n",
      "2024-04-05  30.920000  31.559999  30.559999  30.750000  3302295\n",
      "2024-04-08  30.850000  31.240000  30.830000  30.959999  1588796\n"
     ]
    }
   ],
   "source": [
    "# load data for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    # create name for the CSV\n",
    "    csv_file = f'stock_data_{symbol}_2019.csv'\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Select the desired column\n",
    "    selected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "    df = df[selected_columns]\n",
    "    \n",
    "    # show data\n",
    "    print(f\"Daten für Symbol {symbol}:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08973d9b",
   "metadata": {},
   "source": [
    "# Compare predictions to actual share values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbe67132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vergleich für Symbol ALV.DE:\n",
      "                Actual   Predicted\n",
      "2024-04-29  266.299988  145.755386\n",
      "MSE für Symbol ALV.DE: 14531.000936432043\n",
      "\n",
      "\n",
      "Vergleich für Symbol DBK.DE:\n",
      "            Actual  Predicted\n",
      "2024-04-29  15.104  14.388221\n",
      "MSE für Symbol DBK.DE: 0.5123400127567947\n",
      "\n",
      "\n",
      "Vergleich für Symbol VOW3.DE:\n",
      "            Actual  Predicted\n",
      "2024-04-29  120.75  85.346313\n",
      "MSE für Symbol VOW3.DE: 1253.42101944983\n",
      "\n",
      "\n",
      "Vergleich für Symbol BMW.DE:\n",
      "                Actual  Predicted\n",
      "2024-04-29  106.800003  87.201279\n",
      "MSE für Symbol BMW.DE: 384.109996744431\n",
      "\n",
      "\n",
      "Vergleich für Symbol ADS.DE:\n",
      "                Actual   Predicted\n",
      "2024-04-29  232.300003  108.782417\n",
      "MSE für Symbol ADS.DE: 15256.593990594207\n",
      "\n",
      "\n",
      "Vergleich für Symbol BEI.DE:\n",
      "                Actual   Predicted\n",
      "2024-04-29  139.449997  100.752472\n",
      "MSE für Symbol BEI.DE: 1497.4984430151526\n",
      "\n",
      "\n",
      "Vergleich für Symbol DTE.SG:\n",
      "               Actual  Predicted\n",
      "2024-04-29  21.719999  20.497587\n",
      "MSE für Symbol DTE.SG: 1.4942913651466283\n",
      "\n",
      "\n",
      "Vergleich für Symbol SAP.DE:\n",
      "                Actual  Predicted\n",
      "2024-04-29  171.419998  84.601852\n",
      "MSE für Symbol SAP.DE: 7537.390431807376\n",
      "\n",
      "\n",
      "Vergleich für Symbol 1COV.DE:\n",
      "               Actual  Predicted\n",
      "2024-04-29  47.549999  49.830753\n",
      "MSE für Symbol 1COV.DE: 5.2018392161116935\n",
      "\n",
      "\n",
      "Vergleich für Symbol BAS.DE:\n",
      "            Actual  Predicted\n",
      "2024-04-29  49.055  50.684361\n",
      "MSE für Symbol BAS.DE: 2.6548146580025787\n",
      "\n",
      "\n",
      "Vergleich für Symbol EOAN.DE:\n",
      "            Actual  Predicted\n",
      "2024-04-29  12.465  13.794801\n",
      "MSE für Symbol EOAN.DE: 1.7683696511167\n",
      "\n",
      "\n",
      "Vergleich für Symbol RWE.DE:\n",
      "            Actual  Predicted\n",
      "2024-04-29    33.0  31.657511\n",
      "MSE für Symbol RWE.DE: 1.8022773663724365\n",
      "\n",
      "\n",
      "Gesamter Mean Squared Error für alle Vorhersagen: 3372.7873958593786\n",
      "{'ALV.DE': {'prediction_20_days_ahead': 145.75539}, 'DBK.DE': {'prediction_20_days_ahead': 14.388221}, 'VOW3.DE': {'prediction_20_days_ahead': 85.34631}, 'BMW.DE': {'prediction_20_days_ahead': 87.20128}, 'ADS.DE': {'prediction_20_days_ahead': 108.78242}, 'BEI.DE': {'prediction_20_days_ahead': 100.75247}, 'DTE.SG': {'prediction_20_days_ahead': 20.497587}, 'SAP.DE': {'prediction_20_days_ahead': 84.60185}, '1COV.DE': {'prediction_20_days_ahead': 49.830753}, 'BAS.DE': {'prediction_20_days_ahead': 50.68436}, 'EOAN.DE': {'prediction_20_days_ahead': 13.794801}, 'RWE.DE': {'prediction_20_days_ahead': 31.65751}}\n"
     ]
    }
   ],
   "source": [
    "# list to store the mse (mean squared error) for every symbol\n",
    "mse_list = []\n",
    "\n",
    "# list to store the actual and predicted value for every symbol\n",
    "all_actual_values = []\n",
    "all_predicted_values = []\n",
    "\n",
    "# Comparison of the actual and the predicted value for every symbol\n",
    "for symbol in stock_symbols:\n",
    "    # create name for the CSV\n",
    "    csv_file = f'stock_data_{symbol}_2019.csv'\n",
    "    \n",
    "    # load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # set date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # extract last row\n",
    "    last_row = df.iloc[-1]\n",
    "    \n",
    "    # Extract the value from the \"Close\" column of the last row\n",
    "    actual_close = last_row['Close']\n",
    "    \n",
    "    # Calculate the mean squared error (mse)\n",
    "    mse = mean_squared_error([actual_close], [predictions_dict[symbol]['prediction_20_days_ahead']])\n",
    "    \n",
    "    # Add mse to the list\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    # Create a data frame with just one row for the actual and the predicted \"Close\" value\n",
    "    comparison_df = pd.DataFrame({'Actual': [actual_close], 'Predicted': predictions_dict[symbol]['prediction_20_days_ahead']}, index=[last_row.name])\n",
    "    \n",
    "    # Output of the comparison\n",
    "    print(f\"Vergleich für Symbol {symbol}:\")\n",
    "    print(comparison_df)\n",
    "    print(f\"MSE für Symbol {symbol}: {mse}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Add the actual and predicted value to the main list\n",
    "    all_actual_values.append(actual_close)\n",
    "    all_predicted_values.append(predictions_dict[symbol]['prediction_20_days_ahead'])\n",
    "    \n",
    "# calculate the mean squared error (mse) for all predictions\n",
    "total_mse = mean_squared_error(all_actual_values, all_predicted_values)\n",
    "\n",
    "# Output of the mse for all values\n",
    "print(f\"Gesamter Mean Squared Error für alle Vorhersagen: {total_mse}\")\n",
    "\n",
    "# Define custom encoder class to handle float32 values\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "# Convert NumPy arrays in lists (if present)\n",
    "predictions_dict_serializable = {symbol: predictions.tolist() if isinstance(predictions, np.ndarray) else predictions for symbol, predictions in predictions_dict.items()}\n",
    "print(predictions_dict_serializable)\n",
    "# extract predictions for the mean variance model\n",
    "json_file = 'settings/predictions_dict.json'\n",
    "\n",
    "# Write array in JSON file\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(predictions_dict_serializable, f, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45dd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
