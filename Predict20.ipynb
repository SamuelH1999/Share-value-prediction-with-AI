{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac4dcf2",
   "metadata": {},
   "source": [
    "# Import librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64238bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.metrics import MeanSquaredError\n",
    "from sklearn.base import BaseEstimator\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import keras\n",
    "from keras.callbacks import History\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import RMSprop\n",
    "import yfinance as yf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f82c3",
   "metadata": {},
   "source": [
    "# CSV einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86ee6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(stock_symbols, start_date, end_date, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write column headings\n",
    "        f.write('Date,Open,High,Low,Close,Volume\\n')\n",
    "        \n",
    "        for symbol in stock_symbols:\n",
    "            # Load the share data for every symbol and the defined time period\n",
    "            stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "            stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Auswahl der gewünschten Spalten\n",
    "            stock_data.to_csv(f, header=False)  # Schreiben der Daten in die Datei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77172612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Define time period and symbol\n",
    "stock_symbols = ['RWE.DE']\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-12-31'\n",
    "output_file = 'stock_data.csv'\n",
    "\n",
    "# Load and store the share data\n",
    "download_stock_data(stock_symbols, start_date, end_date, output_file)\n",
    "\n",
    "# Search and delete empty rows in the CSV file\n",
    "with open(output_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Filter empty rows\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Overwrite the file with the cleaned rows\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5950e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2000-01-03  39.568237  40.066578  38.422054  38.671223  1236054\n",
      "2000-01-04  38.970230  38.970230  36.986832  38.681190  1448199\n",
      "2000-01-05  36.857265  38.521721  36.378857  37.415405  1475345\n",
      "2000-01-06  37.375538  38.372219  37.186169  38.162918   935343\n",
      "2000-01-07  38.471889  38.870560  37.176205  37.365574  1538811\n",
      "6137\n"
     ]
    }
   ],
   "source": [
    "# load CSV\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# transform date to index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Show first rows\n",
    "print(df.head())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563e76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte vor der Behandlung gefunden: False\n",
      "Fehlende Werte nach der Behandlung gefunden: False\n"
     ]
    }
   ],
   "source": [
    "# Check, if there are any missing numbers\n",
    "missing_values_before = df.isnull().values.any()\n",
    "\n",
    "# Treat missing values with the mean of the previous and the following row\n",
    "for column in df.columns:\n",
    "    missing_values = df[column].isnull()\n",
    "    df.loc[missing_values, column] = (df[column].shift() + df[column].shift(-1)) / 2\n",
    "\n",
    "# Check, if there are missing values after the Treatment\n",
    "missing_values_after = df.isnull().values.any()\n",
    "\n",
    "# Output of the missing values before and after the Treatment\n",
    "print(\"Fehlende Werte vor der Behandlung gefunden:\", missing_values_before)\n",
    "print(\"Fehlende Werte nach der Behandlung gefunden:\", missing_values_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349e97c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2000-01-03  39.568237  40.066578  38.422054  38.671223  1236054\n",
      "2000-01-04  38.970230  38.970230  36.986832  38.681190  1448199\n",
      "2000-01-05  36.857265  38.521721  36.378857  37.415405  1475345\n",
      "2000-01-06  37.375538  38.372219  37.186169  38.162918   935343\n",
      "2000-01-07  38.471889  38.870560  37.176205  37.365574  1538811\n"
     ]
    }
   ],
   "source": [
    "# show first rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d732ee",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b22f89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low     Close    Volume\n",
      "2000-01-03  0.334060  0.327197  0.328187  0.323453  0.029513\n",
      "2000-01-04  0.327495  0.315326  0.312109  0.323563  0.034579\n",
      "2000-01-05  0.304302  0.310469  0.305298  0.309668  0.035227\n",
      "2000-01-06  0.309991  0.308850  0.314342  0.317874  0.022333\n",
      "2000-01-07  0.322025  0.314246  0.314230  0.309121  0.036742\n",
      "Länge des Datensatzes: 6137\n"
     ]
    }
   ],
   "source": [
    "# Copy the data frame and remove the column \"date\"\n",
    "nf = df.copy()\n",
    "\n",
    "# Remove the index name\n",
    "nf.index.name = None\n",
    "\n",
    "# normalize the data, except the \"date\" (index)\n",
    "scaler = MinMaxScaler()\n",
    "nf_normalized = scaler.fit_transform(nf)\n",
    "\n",
    "# Create a new data frame with the normalized data and the original index\n",
    "nf = pd.DataFrame(nf_normalized, columns=nf.columns, index=nf.index)\n",
    "\n",
    "# Show the normalized data\n",
    "print(nf.head())\n",
    "\n",
    "# Show the length of the data frame\n",
    "print(\"Länge des Datensatzes:\", len(nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47704f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69186c2e",
   "metadata": {},
   "source": [
    "# read predicting sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074f3def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Define time period and output file\n",
    "start_date = '2023-10-01'\n",
    "end_date = '2023-12-31'\n",
    "output_file = 'stock_data_pre.csv'\n",
    "\n",
    "# Load and store the share data\n",
    "download_stock_data(stock_symbols, start_date, end_date, output_file)\n",
    "\n",
    "# Search and delete the empty rows in the CSV file\n",
    "with open(output_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Filter the empty rows\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Overwrite the file with the cleaned rows\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e1a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Open       High        Low      Close   Volume\n",
      "Date                                                           \n",
      "2023-10-05  33.000000  33.189999  32.869999  33.040001  1641405\n",
      "2023-10-06  33.040001  33.099998  31.549999  32.730000  2679175\n",
      "2023-10-09  32.900002  33.349998  32.799999  33.090000  1903629\n",
      "2023-10-10  33.549999  34.480000  33.549999  34.270000  2932864\n",
      "2023-10-11  34.389999  35.049999  34.230000  34.630001  2509883\n"
     ]
    }
   ],
   "source": [
    "# load CSV\n",
    "dfPre = pd.read_csv('stock_data_pre.csv')\n",
    "\n",
    "# Drop empty rows\n",
    "dfPre.dropna(inplace=True)\n",
    "\n",
    "# transform date to index\n",
    "dfPre.set_index('Date', inplace=True)\n",
    "\n",
    "# Check the number of rows\n",
    "num_rows = dfPre.shape[0]\n",
    "\n",
    "# If more than 60 rows, keep the last 60 rows\n",
    "if num_rows > 60:\n",
    "    dfPre = dfPre.tail(60)\n",
    "\n",
    "# Show first rows\n",
    "print(dfPre.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b63ed050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehlende Werte vor der Behandlung gefunden: False\n",
      "Fehlende Werte nach der Behandlung gefunden: False\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# Check, if there are missing values before the treatment\n",
    "missing_values_beforePre = dfPre.isnull().values.any()\n",
    "\n",
    "# Treat missing values for all columns with the mean of the previous and following row\n",
    "for column in df.columns:\n",
    "    missing_valuesPre = dfPre[column].isnull()\n",
    "    dfPre.loc[missing_valuesPre, column] = (dfPre[column].shift() + dfPre[column].shift(-1)) / 2\n",
    "\n",
    "# Check, if there are missing values after the treatment\n",
    "missing_values_afterPre = dfPre.isnull().values.any()\n",
    "\n",
    "# Output of the missing values before and after the treatment\n",
    "print(\"Fehlende Werte vor der Behandlung gefunden:\", missing_values_beforePre)\n",
    "print(\"Fehlende Werte nach der Behandlung gefunden:\", missing_values_afterPre)\n",
    "\n",
    "# show lenth of data set\n",
    "print(len(dfPre))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6894f0f",
   "metadata": {},
   "source": [
    "# Data normalization prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda529dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des Datensatzes: 60\n"
     ]
    }
   ],
   "source": [
    "# Copy the data frame and remove the column \"date\"\n",
    "nfPre = dfPre.copy()\n",
    "\n",
    "# Remove the index name\n",
    "nfPre.index.name = None\n",
    "\n",
    "# Normalize the data, except the \"date\" (Index)\n",
    "nfPre_normalized = scaler.fit_transform(nfPre)\n",
    "\n",
    "# Create a data frame with the normalized data and the original index\n",
    "nfPre = pd.DataFrame(nfPre_normalized, columns=nfPre.columns, index=nfPre.index)\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "nfPre = nfPre.to_numpy()\n",
    "\n",
    "# Show length of the data frame\n",
    "print(\"Länge des Datensatzes:\", len(nfPre))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc3682",
   "metadata": {},
   "source": [
    "# Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4489b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequenences for LSTM model\n",
    "def create_sequences(data_input, seq_length):\n",
    "    data = data_input\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - 20):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data.iloc[i+seq_length+20]) # acess to Close-Value of the 20th row\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305f39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequence\n",
    "sequence_length = 60\n",
    "X, y = create_sequences(nf, sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d71c64",
   "metadata": {},
   "source": [
    "# Split the data in Training, Test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8deee9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten: 3927\n",
      "Validierungsdaten: 982\n",
      "Testdaten: 1228\n"
     ]
    }
   ],
   "source": [
    "# Split data in training and test (80% training, 20% test)\n",
    "train_data, test_data = train_test_split(nf, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split data in training and validation (80% training, 20% validation)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Split the train data in sequences\n",
    "X_train, y_train = create_sequences(train_data, sequence_length)\n",
    "\n",
    "# Split the validation data in sequences\n",
    "X_val, y_val = create_sequences(val_data, sequence_length)\n",
    "\n",
    "# Split the test data in sequences\n",
    "X_test, y_test = create_sequences(test_data, sequence_length)\n",
    "\n",
    "# show training, validation and test sets\n",
    "print(\"Trainingsdaten:\", len(train_data))\n",
    "print(\"Validierungsdaten:\", len(val_data))\n",
    "print(\"Testdaten:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7599c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train_size, val_size and test_size\n",
    "train_size = len(train_data)\n",
    "val_size = len(val_data)\n",
    "test_size = len(test_data)\n",
    "nfPre_size = len(nfPre)\n",
    "\n",
    "# Extract features and target variables for training, validation, and test sets\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "# Add a external dimension\n",
    "X_pre = np.expand_dims(nfPre[:nfPre_size], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da05eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the train, val and prediction data into tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "X_pre_tensor = tf.convert_to_tensor(X_pre, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a276f39",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "182d8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyper parameters\n",
    "units = 25\n",
    "learning_rate = 0.003\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "dropout_rate = 0.5\n",
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020b49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "input_layer = Input(shape=(sequence_length, 5))\n",
    "\n",
    "# create model\n",
    "model = Sequential([\n",
    "    LSTM(units=units, return_sequences=True),\n",
    "    Dropout(dropout_rate),\n",
    "    LSTM(units=units, return_sequences=False),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(units=5)  # Anpassen der Anzahl der Ausgabeneinheiten auf 5 für 5 Input-Variablen\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab1567",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5f2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 0.0591 - val_loss: 0.0036\n",
      "Epoch 2/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0162 - val_loss: 0.0023\n",
      "Epoch 3/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0118 - val_loss: 0.0018\n",
      "Epoch 4/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0097 - val_loss: 0.0021\n",
      "Epoch 5/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0083 - val_loss: 0.0017\n",
      "Epoch 6/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0065 - val_loss: 0.0021\n",
      "Epoch 7/10\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0060 - val_loss: 0.0022\n",
      "Epoch 8/10\n",
      "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0055"
     ]
    }
   ],
   "source": [
    "# Retrieve the history object\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
    "\n",
    "# Show the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Plot the loss function during the training\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0705d",
   "metadata": {},
   "source": [
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in stock_symbols:\n",
    "    model.save(f'Predict20Days2024_{symbol}.h5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df86dc",
   "metadata": {},
   "source": [
    "# Predictions X-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba1965",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(f'Prediction: {predictions}')\n",
    "print(len(predictions))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ea732",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(predictions[:,-2], color=\"green\", label=\"Prediction\")\n",
    "plt.plot(y_test[:,-2], color=\"black\", label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb2a5e",
   "metadata": {},
   "source": [
    "# Compare test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90458c92",
   "metadata": {},
   "source": [
    "# Prediction for the unkonown sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09634d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_pre)\n",
    "prediction_20_days_ahead = predictions[0][-2]\n",
    "print(f'Prediction: {predictions}')\n",
    "print(len(predictions))\n",
    "print(len(X_pre))\n",
    "print(prediction_20_days_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5392bcc",
   "metadata": {},
   "source": [
    "# Transform back into monetary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transformation of the predictions\n",
    "predictions_original_scale = scaler.inverse_transform(predictions)\n",
    "print(predictions_original_scale)\n",
    "prediction_20_days_ahead = predictions_original_scale[0][-2]\n",
    "\n",
    "# shape a 1-Dimensional array\n",
    "predictions_close = predictions_original_scale[:, -1]  # Extract only the close values\n",
    "\n",
    "# Show values of inverse transformation\n",
    "print(\"Zurücktransformierte Vorhersagen:\")\n",
    "print(predictions_original_scale)\n",
    "print(prediction_20_days_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a4f65",
   "metadata": {},
   "source": [
    "# read test CSV from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time period and output file\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2019-01-31'\n",
    "output_file = 'stock_data_2019.csv'\n",
    "\n",
    "# Load and store the share data\n",
    "download_stock_data(stock_symbols, start_date, end_date, output_file)\n",
    "\n",
    "# Search and delete empty rows in the CSV file\n",
    "with open(output_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Filter empty rows\n",
    "lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "# Overwrite the file with the cleaned data\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no normalization. The data will be compared to the prediction data from the LSTM\n",
    "# load CSV\n",
    "df2019 = pd.read_csv('stock_data_2019.csv')\n",
    "\n",
    "# transform date to index\n",
    "df2019.set_index('Date', inplace=True)\n",
    "\n",
    "print(df2019.head())\n",
    "\n",
    "# Select columns \"Date\" and \"Close\"\n",
    "selected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df2019 = df2019[selected_columns]\n",
    "\n",
    "# Show first rows\n",
    "print(df2019.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08973d9b",
   "metadata": {},
   "source": [
    "# Compare predictions to actual share values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe67132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last row\n",
    "last_row = df2019.iloc[-1]\n",
    "# extract the \"Close\" value of the last row\n",
    "actual_close = last_row['Close']\n",
    "# Create a DataFrame with only one entry for the actual closing price and on entry for the predicted closing price\n",
    "comparison_df = pd.DataFrame({'Actual': [actual_close], 'Predicted': prediction_20_days_ahead}, index=[last_row.name])\n",
    "# Show comparison\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b246b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
