{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19628bf5",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c450942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c5927e",
   "metadata": {},
   "source": [
    "# function to download stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6f2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(stock_symbols, start_date, end_date, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        # Write heading of the columns\n",
    "        f.write('Date,Open,High,Low,Close,Volume\\n')\n",
    "        # load share data for the time period needed\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Choose the right columns\n",
    "        stock_data.to_csv(f, header=False)  # Write the data to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea319ae",
   "metadata": {},
   "source": [
    "# Define the shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1782716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_symbols = ['ALV.DE', 'DBK.DE', 'VOW3.DE', 'BMW.DE', 'ADS.DE', 'BEI.DE', 'DTE.SG', 'SAP.DE', '1COV.DE', 'BAS.DE', 'EOAN.DE', 'RWE.DE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e64b8c",
   "metadata": {},
   "source": [
    "# Read start and end for the sequence, which will be predicted from Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c7ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Startdatum: 2024-03-01, Enddatum: 2024-03-31\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/actualMonth_startEnd.xlsx'\n",
    "\n",
    "# Path to file, in which the index is stored\n",
    "index_file_path = 'settings/actualMonthIndex.txt'\n",
    "\n",
    "# Read the current index from the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "print(current_row_index)\n",
    "# read excel file\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if the index is out of bounds\n",
    "if current_row_index >= len(df):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract the data from the current row\n",
    "    start_date = df.loc[current_row_index, 'start_date']\n",
    "    end_date = df.loc[current_row_index, 'end_date']\n",
    "\n",
    "    # Output of the current data\n",
    "    print(f'Startdatum: {start_date}, Enddatum: {end_date}')\n",
    "\n",
    "    # Updating the index in the index file for the next call\n",
    "    with open(index_file_path, 'w') as index_file:\n",
    "        index_file.write(str(current_row_index + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882aaec1",
   "metadata": {},
   "source": [
    "# Read close values at the begin of the time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057ba5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-25 00:00:00\n",
      "2024-02-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Definition of the time period\n",
    "# convert in to datetime objekt to substract days for the last day before this period\n",
    "start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "# Substract 5 days for the new start_date and 1 day for the new end_date\n",
    "# In the next block the last date will be extracted \n",
    "new_start_date = start_date_obj - timedelta(days=5)\n",
    "new_end_date = start_date_obj - timedelta(days=1)\n",
    "\n",
    "print(new_start_date)\n",
    "print(new_end_date)\n",
    "\n",
    "# Load and store the data for every symbol (share)\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_begin{symbol}.csv'\n",
    "    download_stock_data(symbol, new_start_date, new_end_date, output_file)\n",
    "\n",
    "    # Search and delete empty rows in the CSV file\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter the empty rows\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the file with the cleaned data\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288520ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Symbol  Close_Value\n",
      "0    ALV.DE   248.550003\n",
      "1    DBK.DE    12.396000\n",
      "2   VOW3.DE   125.879997\n",
      "3    BMW.DE   109.160004\n",
      "4    ADS.DE   189.839996\n",
      "5    BEI.DE   137.699997\n",
      "6    DTE.SG    21.959999\n",
      "7    SAP.DE   172.119995\n",
      "8   1COV.DE    49.430000\n",
      "9    BAS.DE    46.970001\n",
      "10  EOAN.DE    11.735000\n",
      "11   RWE.DE    30.790001\n"
     ]
    }
   ],
   "source": [
    "# create data frame for the collected data \n",
    "combined_data_begin = pd.DataFrame(columns=['Symbol', 'Close_Value'])\n",
    "\n",
    "# load and store the data for every symbol (share)\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_begin{symbol}.csv'\n",
    "    # read CSV file \n",
    "    df = pd.read_csv(output_file)\n",
    "\n",
    "    # Select the last row only\n",
    "    last_row = df.tail(1)\n",
    "\n",
    "     # Write symbol and close value to the data frame\n",
    "    data = {'Symbol': symbol, 'Close_Value': last_row['Close'].iloc[0]}\n",
    "    combined_data_begin = pd.concat([combined_data_begin, pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "print(combined_data_begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69294cef",
   "metadata": {},
   "source": [
    "# read predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc05f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              ALV.DE     DBK.DE    VOW3.DE     BMW.DE  \\\n",
      "prediction_20_days_ahead  143.810287  14.912164  86.344261  86.055275   \n",
      "\n",
      "                              ADS.DE      BEI.DE     DTE.SG     SAP.DE  \\\n",
      "prediction_20_days_ahead  105.564362  102.767632  20.717569  82.988327   \n",
      "\n",
      "                            1COV.DE     BAS.DE    EOAN.DE    RWE.DE  \n",
      "prediction_20_days_ahead  48.966732  46.389107  13.493193  32.02832  \n"
     ]
    }
   ],
   "source": [
    "# Define the name of the file\n",
    "json_file = 'settings/predictions_dict.json'\n",
    "\n",
    "# read array from the json file\n",
    "with open(json_file, 'r') as f:\n",
    "    predictions_dict = json.load(f)\n",
    "    \n",
    "# Convert predictions_dict to DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_dict)\n",
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6e4bc",
   "metadata": {},
   "source": [
    "# Calculate return in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5eaaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             ALV.DE     DBK.DE    VOW3.DE     BMW.DE  \\\n",
      "prediction_20_days_ahead -42.140299  20.298192 -31.407481 -21.165929   \n",
      "\n",
      "                             ADS.DE     BEI.DE    DTE.SG    SAP.DE   1COV.DE  \\\n",
      "prediction_20_days_ahead -44.392982 -25.368458 -5.657695 -51.78461 -0.937221   \n",
      "\n",
      "                            BAS.DE    EOAN.DE    RWE.DE  \n",
      "prediction_20_days_ahead -1.236735  14.982472  4.021823  \n"
     ]
    }
   ],
   "source": [
    "# FÃ¼r jedes Symbol in predictions_df den entsprechenden Close_Value aus combined_data_begin abziehen\n",
    "for symbol in predictions_df.columns:\n",
    "    if symbol in combined_data_begin['Symbol'].values:\n",
    "        close_value = combined_data_begin.loc[combined_data_begin['Symbol'] == symbol, 'Close_Value'].values[0]\n",
    "        predictions_df[symbol] = ((predictions_df[symbol] - close_value) / close_value) * 100\n",
    "\n",
    "print(predictions_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad3721",
   "metadata": {},
   "source": [
    "# read profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78181960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit: -2.0761984664113697\n"
     ]
    }
   ],
   "source": [
    "json_file_path = 'settings/profit.json'\n",
    "\n",
    "try:\n",
    "    # read JSON file\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        profit = data['profit']\n",
    "except FileNotFoundError:\n",
    "    # If the file can not be found, set the value of new_portfolio_value to 0\n",
    "    profit = 0\n",
    "\n",
    "print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38ceb3",
   "metadata": {},
   "source": [
    "# load share value from 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f606101a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startdatum: 2024-03-01, Enddatum: 2024-03-31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "start_date = start_date\n",
    "end_date = end_date\n",
    "print(f'Startdatum: {start_date}, Enddatum: {end_date}')\n",
    "# Load and store the symbols for every symbol (share)\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}_2019.csv'\n",
    "    download_stock_data(symbol, start_date, end_date, output_file)\n",
    "\n",
    "    # search and delete for empty rows in the CSV \n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter the empty rows\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the file with the cleaned rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee54ec1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277.79998779296875\n",
      "Liste der letzten Close-Werte:\n",
      "ALV.DE: 277.79998779296875\n",
      "DBK.DE: 14.58199977874756\n",
      "VOW3.DE: 122.83999633789062\n",
      "BMW.DE: 106.95999908447266\n",
      "ADS.DE: 207.0\n",
      "BEI.DE: 134.9499969482422\n",
      "DTE.SG: 22.434999465942383\n",
      "SAP.DE: 180.4600067138672\n",
      "1COV.DE: 50.68000030517578\n",
      "BAS.DE: 52.93000030517578\n",
      "EOAN.DE: 12.885000228881836\n",
      "RWE.DE: 31.459999084472656\n"
     ]
    }
   ],
   "source": [
    "# list for the las close value of the shares\n",
    "last_close_values = []\n",
    "\n",
    "# load for each symbol\n",
    "for symbol in stock_symbols:\n",
    "    # file name of the csv data\n",
    "    csv_file = f'stock_data_{symbol}_2019.csv'\n",
    "    \n",
    "    # load CSV data\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # set date as index\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # collect just the last close value and write it to the list\n",
    "    last_close_value = df['Close'].iloc[-1]\n",
    "    last_close_values.append((symbol, last_close_value))\n",
    "print(last_close_values[0][1])\n",
    "# print the list\n",
    "print(\"Liste der letzten Close-Werte:\")\n",
    "for symbol, last_close_value in last_close_values:\n",
    "    print(f\"{symbol}: {last_close_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6215f",
   "metadata": {},
   "source": [
    "# Historic data from 2018 for the covarianz matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b131f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startdatum: 2023-10-01, Enddatum: 2024-02-28\n"
     ]
    }
   ],
   "source": [
    "excel_file_path = 'settings/lastSequenceForPrediction.xlsx'\n",
    "\n",
    "# path to the data, in which the index is stored\n",
    "index_file_path = 'settings/lastSequenceForPrediction.txt'\n",
    "\n",
    "# read the current index from the index file\n",
    "try:\n",
    "    with open(index_file_path, 'r') as index_file:\n",
    "        current_row_index = int(index_file.read().strip())\n",
    "except FileNotFoundError:\n",
    "    current_row_index = 0\n",
    "\n",
    "# read excel file\n",
    "df_lastSeq = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Check, if index is out of bounds\n",
    "if current_row_index >= len(df_lastSeq):\n",
    "    print(\"Es gibt keine weiteren Zeilen in der Excel-Tabelle.\")\n",
    "else:\n",
    "    # extract data from the current row\n",
    "    start_date_lastSeq = df_lastSeq.loc[current_row_index, 'start_date']\n",
    "    end_date_lastSeq = df_lastSeq.loc[current_row_index, 'end_date']\n",
    "    \n",
    "    # Output of the current data \n",
    "    print(f'Startdatum: {start_date_lastSeq}, Enddatum: {end_date_lastSeq}')\n",
    "\n",
    "    # Update the index in the index file for the next call\n",
    "    with open(index_file_path, 'w') as index_file:\n",
    "        index_file.write(str(current_row_index + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b06561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Definition of the time period\n",
    "start_date2018 = '2023-01-01'\n",
    "end_date2018 = end_date_lastSeq\n",
    "print(end_date2018)\n",
    "# Load and store the data for every symbol (share)\n",
    "for symbol in stock_symbols:\n",
    "    output_file = f'stock_data_{symbol}_2018.csv'\n",
    "    download_stock_data(symbol, start_date2018, end_date2018, output_file)\n",
    "\n",
    "    # Search and delete empty rows in the CSV file\n",
    "    with open(output_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Filter the empty rows\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Overwrite the file with the cleaned rows\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write('\\n'.join(lines))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf44c1",
   "metadata": {},
   "source": [
    "# Combine historical data to one file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effaa43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ALV.DE  DBK.DE     VOW3.DE      BMW.DE      ADS.DE  \\\n",
      "2023-01-02  203.050003  10.942  120.040001   85.800003  127.699997   \n",
      "2023-01-03  205.199997  11.112  122.059998   85.830002  131.880005   \n",
      "2023-01-04  211.500000  11.698  125.879997   87.879997  138.380005   \n",
      "2023-01-05  209.550003  11.494  127.120003   88.949997  138.539993   \n",
      "2023-01-06  211.800003  11.596  128.160004   89.529999  140.679993   \n",
      "...                ...     ...         ...         ...         ...   \n",
      "2024-02-21  250.699997  11.994  119.580002  104.300003  181.440002   \n",
      "2024-02-22  255.100006  12.252  122.400002  105.080002  187.820007   \n",
      "2024-02-23  246.500000  12.390  124.000000  106.720001  188.779999   \n",
      "2024-02-26  245.649994  12.302  123.019997  107.480003  188.199997   \n",
      "2024-02-27  248.000000  12.384  124.080002  107.680000  188.160004   \n",
      "\n",
      "                BEI.DE     DTE.SG      SAP.DE    1COV.DE     BAS.DE  EOAN.DE  \\\n",
      "2023-01-02  107.150002  18.910000   97.419998  38.259998  47.985001    9.468   \n",
      "2023-01-03  107.500000  19.056000   98.510002  39.580002  48.889999    9.656   \n",
      "2023-01-04  108.800003  19.184000  100.699997  41.470001  50.959999    9.992   \n",
      "2023-01-05  107.800003  19.507999  100.839996  41.709999  51.619999    9.796   \n",
      "2023-01-06  108.300003  19.754000  102.339996  42.369999  52.810001    9.878   \n",
      "...                ...        ...         ...        ...        ...      ...   \n",
      "2024-02-21  139.550003  22.205000  162.279999  47.570000  46.404999   11.980   \n",
      "2024-02-22  140.600006  22.395000  167.220001  49.830002  46.845001   11.805   \n",
      "2024-02-23  140.750000  22.070000  169.800003  50.000000  46.599998   11.800   \n",
      "2024-02-26  140.949997  22.035000  173.000000  50.080002  46.330002   11.740   \n",
      "2024-02-27  139.100006  22.160000  174.800003  49.959999  47.455002   11.700   \n",
      "\n",
      "               RWE.DE  \n",
      "2023-01-02  41.590000  \n",
      "2023-01-03  39.619999  \n",
      "2023-01-04  38.669998  \n",
      "2023-01-05  38.889999  \n",
      "2023-01-06  39.139999  \n",
      "...               ...  \n",
      "2024-02-21  31.180000  \n",
      "2024-02-22  31.110001  \n",
      "2024-02-23  30.969999  \n",
      "2024-02-26  30.360001  \n",
      "2024-02-27  31.010000  \n",
      "\n",
      "[296 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data frame for the combined data\n",
    "combined_historical2018_data = pd.DataFrame()\n",
    "\n",
    "# load and combine the data for each share\n",
    "for symbol in stock_symbols:\n",
    "    # file name for the csv file\n",
    "    csv_file = f'stock_data_{symbol}_2018.csv'\n",
    "    \n",
    "    # read CSV file and set date to index\n",
    "    df = pd.read_csv(csv_file, index_col='Date', parse_dates=True)\n",
    "    \n",
    "    # change name of the close column\n",
    "    df.rename(columns={'Close': symbol}, inplace=True)\n",
    "    \n",
    "    # write data to the combined file\n",
    "    combined_historical2018_data = pd.concat([combined_historical2018_data, df[symbol]], axis=1)\n",
    "\n",
    "# drop rows with NaN values\n",
    "combined_historical2018_data.dropna(inplace=True)\n",
    "    \n",
    "# create a CSV file for the data\n",
    "combined_historical2018_data.to_csv('combined_historical2018_stock_data.csv')\n",
    "\n",
    "print(combined_historical2018_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428f38c",
   "metadata": {},
   "source": [
    "# Mean variance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d4a0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_variance_optimization(expected_returns, covariance_matrix):\n",
    "    n = len(expected_returns)\n",
    "    initial_weights = np.array([1/n] * n)  # start weighs\n",
    "    bounds = [(0, 1)] * n  # weigh border (0-100% for each share)\n",
    "\n",
    "    # Minimize the negative sharpe ratio\n",
    "    def negative_sharpe(weights, expected_returns, covariance_matrix):\n",
    "        portfolio_return = np.dot(weights, expected_returns)\n",
    "        portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
    "        return -portfolio_return / portfolio_volatility\n",
    "\n",
    "    result = minimize(negative_sharpe, initial_weights, args=(expected_returns, covariance_matrix), bounds=bounds, constraints={'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718e459",
   "metadata": {},
   "source": [
    "# Portfilio allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb99f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_portfolio(predictions, historical_data, initial_capital):\n",
    "    # Calculate expected returns based on the predictions\n",
    "    expected_returns = predictions.mean()\n",
    "    \n",
    "    # Calculate the covariance matrix of the returns\n",
    "    covariance_matrix = historical_data.cov()\n",
    "    \n",
    "    # Perform mean-variance optimization to obtain optimal weights\n",
    "    optimal_weights = mean_variance_optimization(expected_returns, covariance_matrix)\n",
    "\n",
    "    # Calculate the allocation of assets based on the optimal weights and the available capital\n",
    "    asset_allocation = initial_capital * optimal_weights\n",
    "\n",
    "    return asset_allocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea613d",
   "metadata": {},
   "source": [
    "# Calculate portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93ae54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997.9238015335886\n",
      "[0.00000000e+00 3.79188303e+02 5.04999224e+00 2.10468137e-05\n",
      " 0.00000000e+00 0.00000000e+00 2.18077679e-05 0.00000000e+00\n",
      " 1.37589594e-05 2.87537453e-05 5.77318450e+02 3.63671735e+01]\n"
     ]
    }
   ],
   "source": [
    "initial_capital = 1000 + profit # 1000â¬ start capital\n",
    "print(initial_capital)\n",
    "# Calculate the portfolio allocation\n",
    "portfolio_allocation = allocate_portfolio(predictions_df, combined_historical2018_data, initial_capital)\n",
    "# print portfolio allocation\n",
    "print(portfolio_allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a5cb0",
   "metadata": {},
   "source": [
    "# Simulation for one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1703d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-----------------------------\n",
      "30.589569699724414\n",
      "379.1883031972251\n",
      "379.1883031972251\n",
      "-----------------------------\n",
      "0.04011751148619983\n",
      "5.049992235696798\n",
      "384.23829543292186\n",
      "-----------------------------\n",
      "1.9280700804914174e-07\n",
      "2.1046813704724665e-05\n",
      "384.23831647973554\n",
      "-----------------------------\n",
      "0.0\n",
      "0.0\n",
      "384.23831647973554\n",
      "-----------------------------\n",
      "0.0\n",
      "0.0\n",
      "384.23831647973554\n",
      "-----------------------------\n",
      "9.930677961166368e-07\n",
      "2.1807767893540624e-05\n",
      "384.23833828750344\n",
      "-----------------------------\n",
      "0.0\n",
      "0.0\n",
      "384.23833828750344\n",
      "-----------------------------\n",
      "2.783524037583593e-07\n",
      "1.375895940272211e-05\n",
      "384.23835204646286\n",
      "-----------------------------\n",
      "6.121725469075133e-07\n",
      "2.875374527552684e-05\n",
      "384.2383808002081\n",
      "-----------------------------\n",
      "49.196290339661026\n",
      "577.3184502457162\n",
      "961.5568310459244\n",
      "-----------------------------\n",
      "1.1811358360276671\n",
      "36.367173472654024\n",
      "997.9240045185784\n",
      "-----------------------------\n",
      "Gesamtwert des Portfolios am Ende des ersten Monats von 2019: 997.9240045185784\n",
      "[0.0, 30.589569699724414, 0.04011751148619983, 1.9280700804914174e-07, 0.0, 0.0, 9.930677961166368e-07, 0.0, 2.783524037583593e-07, 6.121725469075133e-07, 49.196290339661026, 1.1811358360276671]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation of the portfolio\n",
    "portfolio_value = 0\n",
    "# initialize the list for the quantity of the shares, which have to be purchased\n",
    "shares_to_buy_list = []\n",
    "# Purchase the shares based on the allocations\n",
    "for i, allocation in enumerate(portfolio_allocation):\n",
    "    # Calculate the quantity of the shares, which should be purchased with this allocation\n",
    "    shares_to_buy = (allocation) / combined_data_begin['Close_Value'][i]\n",
    "    shares_to_buy_list.append(shares_to_buy)\n",
    "    print(shares_to_buy)\n",
    "\n",
    "    # Calculate the value of the stock of the purchased shares\n",
    "    value_of_stock = shares_to_buy * combined_data_begin['Close_Value'][i]\n",
    "    print(value_of_stock)\n",
    "    \n",
    "    # Add the value of the purchased share to the portfolio\n",
    "    portfolio_value += value_of_stock\n",
    "    print(portfolio_value)\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "# Output of the total value of the portfolio at the end of the period to be predicted\n",
    "print(\"Gesamtwert des Portfolios am Ende des ersten Monats von 2019:\", portfolio_value)\n",
    "print(shares_to_buy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ca2fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277.79998779296875\n",
      "0.0\n",
      "0.0\n",
      "----------------------------\n",
      "14.58199977874756\n",
      "30.589569699724414\n",
      "446.0570985933645\n",
      "----------------------------\n",
      "122.83999633789062\n",
      "0.04011751148619983\n",
      "450.9851335574146\n",
      "----------------------------\n",
      "106.95999908447266\n",
      "1.9280700804914174e-07\n",
      "450.985154180052\n",
      "----------------------------\n",
      "207.0\n",
      "0.0\n",
      "450.985154180052\n",
      "----------------------------\n",
      "134.9499969482422\n",
      "0.0\n",
      "450.985154180052\n",
      "----------------------------\n",
      "22.434999465942383\n",
      "9.930677961166368e-07\n",
      "450.9851764595275\n",
      "----------------------------\n",
      "180.4600067138672\n",
      "0.0\n",
      "450.9851764595275\n",
      "----------------------------\n",
      "50.68000030517578\n",
      "2.783524037583593e-07\n",
      "450.98519056642743\n",
      "----------------------------\n",
      "52.93000030517578\n",
      "6.121725469075133e-07\n",
      "450.9852229687205\n",
      "----------------------------\n",
      "12.885000228881836\n",
      "49.196290339661026\n",
      "1084.87943525539\n",
      "----------------------------\n",
      "31.459999084472656\n",
      "1.1811358360276671\n",
      "1122.0379675754582\n",
      "----------------------------\n",
      "Wert des Portfolios:\n",
      "1122.0379675754582\n"
     ]
    }
   ],
   "source": [
    "# Define portfolio value\n",
    "new_portfolio_value = 0\n",
    "# Loop over the indices of the two lists\n",
    "for i in range(len(shares_to_buy_list)):\n",
    "    # convert to float\n",
    "    last_close_value = float(last_close_values[i][1])\n",
    "    print(last_close_value)\n",
    "    print(shares_to_buy_list[i])\n",
    "    #Calculate the new value of the share and add the value to the list\n",
    "    new_portfolio_value += (last_close_value * shares_to_buy_list[i])\n",
    "    print(new_portfolio_value)\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "print(\"Wert des Portfolios:\")\n",
    "print(new_portfolio_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec264d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wert von profit wurde erfolgreich in settings/profit.json gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# just the profit, to store it\n",
    "profit = new_portfolio_value - 1000\n",
    "\n",
    "# path to JSON file\n",
    "json_file_path = 'settings/profit.json'\n",
    "\n",
    "# Create Dictionary with the value of new_portfolio_value\n",
    "data = {'profit': profit}\n",
    "\n",
    "# Write JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file)\n",
    "\n",
    "print(f'Wert von profit wurde erfolgreich in {json_file_path} gespeichert.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d3115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
